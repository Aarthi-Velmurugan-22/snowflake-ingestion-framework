Snowflake Incremental Data Pipeline (S3 â†’ RAW â†’ CURATED)

ğŸ“Œ Project Overview

This project demonstrates an end-to-end **incremental data pipeline** built using **Snowflake**, following real-world data engineering best practices.

The pipeline ingests CSV files from **Amazon S3**, loads them into **RAW staging tables** using **Snowflake Tasks**, captures changes using **Streams**, and applies **CDC logic (Insert / Update / Delete)** into **CURATED tables** using **MERGE statements**.

This project is designed to closely resemble a **production-grade analytics pipeline**.


ğŸ—ï¸ Architecture

The pipeline follows a layered Snowflake architecture:

- Source files land in Amazon S3
- Snowflake Tasks load data into RAW tables
- Streams capture incremental changes
- MERGE logic applies inserts, updates, and deletes
- CURATED tables represent the latest state

ğŸ”§ Technologies Used

* Snowflake (Tasks, Streams, MERGE)
* Amazon S3 (external data source)
* SQL
* CDC (Change Data Capture) design

ğŸ“‚ Data Flow

1ï¸âƒ£ S3 â†’ RAW Layer

* Source CSV files are placed in S3 folders:

  * customer/
  * product/
  * order/

* Snowflake "external stage" points to the S3 bucket
* **Scheduled Snowflake Tasks (every 2 minutes)** load data into RAW tables using `COPY INTO`

RAW tables include metadata columns:

* `OP` â†’ operation type (`I`, `U`, `D`)
* `OP_TS` â†’ operation timestamp


2ï¸âƒ£ RAW â†’ CURATED Layer

* Streams track incremental changes in RAW tables
* MERGE logic applies:

  * Inserts for new records
  * Updates for changed records
  * Deletes when `OP = 'D'`
* CURATED tables exclude operational metadata (`OP`, `OP_TS`)
* CURATED tables represent the latest business state

---

ğŸ—ƒï¸ Schemas & Tables

RAW Schema

* CUSTOMER_STG
* PRODUCT_STG
* ORDER_STG

Includes:

* OP
* OP_TS
* Source columns

CURATED Schema

* CUSTOMER
* PRODUCT
* ORDER

Includes:

* Business columns only
* `CREATED_AT`
* `UPDATED_AT`


ğŸ”„ CDC Logic (MERGE)

CDC is handled using Snowflake `MERGE` statements driven by Streams:

* INSERT â†’ when record does not exist
* UPDATE â†’ when record exists and OP = 'U'
* DELETE â†’ when OP = 'D'

This ensures Snowflake stays in sync with upstream source behavior.


ğŸ“ Repository Structure


snowflake-cdc-pipeline/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ decisions.md
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ create_raw_tables.sql
â”‚   â”‚   â”œâ”€â”€ create_file_format.sql
â”‚   â”‚   â”œâ”€â”€ create_stage.sql
â”‚   â”‚   â””â”€â”€ create_raw_tasks.sql
â”‚   â”‚
â”‚   â”œâ”€â”€ curated/
â”‚   â”‚   â”œâ”€â”€ create_curated_tables.sql
â”‚   â”‚   â”œâ”€â”€ create_streams.sql
â”‚   â”‚   â””â”€â”€ merge_tasks.sql
â”‚
â”œâ”€â”€ sample-data/
â”‚   â”œâ”€â”€ customer.csv
â”‚   â”œâ”€â”€ product.csv
â”‚   â””â”€â”€ order.csv
â”‚
â””â”€â”€ architecture/
    â””â”€â”€ architecture.txt



## ğŸ§  Design Decisions

Key architectural and design choices are documented in **decisions.md**, including:

* Why Tasks were used instead of Snowpipe
* Why Streams + MERGE were chosen
* CDC strategy for deletes
* Separation of RAW and CURATED layers


âœ… Key Learnings

* Designing production-style CDC pipelines
* Handling deletes without hard dependencies on source systems
* Using Snowflake Streams effectively
* Task orchestration and scheduling


ğŸš€ Future Enhancements

* Add error handling and alerting
* Introduce Snowpipe for near real-time ingestion
* Add data quality checks
* Visual architecture diagram


ğŸ‘©â€ğŸ’» Author

**Aarthi Velmurugan**
Data Engineering Portfolio Project

â­ If you like this project, feel free to star the repo!
